# mem0 Docker Stack Environment Variables
# Copy to .env and fill in your values: cp .env.example .env
#
# IMPORTANT: Do NOT wrap values in quotes — Docker .env treats them
# as literal characters (your key would include the quote marks).

# ---- LLM Provider (for fact extraction) ----
# Options: openai, google
MEM0_LLM_PROVIDER=openai
MEM0_LLM_MODEL=gpt-4o-mini

# ---- Embedder Provider (for vector embeddings) ----
# Options: openai, ollama, google
#
# Recommended: use local Ollama to avoid API costs for embeddings.
# The dims MUST match the model:
#   nomic-embed-text:v1.5 (Ollama)   → 768
#   text-embedding-3-small (OpenAI)   → 1536
#   text-embedding-3-large (OpenAI)   → 3072
#
# To use Ollama: install it (https://ollama.com), then:
#   ollama pull nomic-embed-text:v1.5
MEM0_EMBEDDER_PROVIDER=ollama
MEM0_EMBEDDER_MODEL=nomic-embed-text:v1.5
MEM0_EMBEDDING_DIMS=768

# ---- API Keys ----
# Required for OpenAI LLM provider (fact extraction).
# Still needed even when using Ollama for embeddings.
OPENAI_API_KEY=sk-your-key-here

# Required for Google/Gemini provider:
# GEMINI_API_KEY=your-gemini-key-here

# ---- Ollama (local embeddings) ----
# Docker accesses host Ollama via host.docker.internal.
# Default port is 11434; change if your Ollama runs elsewhere.
OLLAMA_BASE_URL=http://host.docker.internal:11434

# ---- Graph Store (Neo4j) ----
# Graph memory stores entity relationships (e.g., "Alice works with Bob").
# Disabled by default because it adds ~21s per memory write (additional LLM calls).
# Enable when you need relationship-aware memory queries.
MEM0_ENABLE_GRAPH=false

# ---- Database Credentials ----
# These match the defaults in docker-compose.yml.
# Change them if you customize the compose file.
# POSTGRES_PASSWORD=mem0password
# NEO4J_PASSWORD=mem0password
